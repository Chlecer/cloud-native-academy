# ‚ö° Performance Engineering Excellence - Enterprise Scale Optimization

## üéØ Objective
Master performance engineering with load testing, capacity planning, performance monitoring, bottleneck identification, and optimization strategies at Fortune 500 scale.

> **"Performance is not just about speed - it's about delivering consistent, reliable experiences under any load."**

## üåü Why Performance Engineering Matters

### Performance Impact Analysis
- **Amazon** - 100ms latency increase = 1% revenue loss ($1.6B annually)
- **Google** - 500ms delay = 20% traffic drop
- **Netflix** - Performance optimization saves $1B+ in infrastructure costs
- **Shopify** - 100ms improvement = 7% conversion increase

### Enterprise Performance Requirements
- **Sub-second response times** under peak load
- **Linear scalability** to handle traffic spikes
- **99.99% availability** during high-traffic events
- **Cost-effective scaling** without over-provisioning
- **Predictive capacity planning** for business growth

## üèóÔ∏è Enterprise Performance Architecture

### Comprehensive Load Testing Framework
```yaml
# performance-testing-platform.yaml - Enterprise load testing
apiVersion: v1
kind: Namespace
metadata:
  name: performance-testing
  labels:
    performance.testing: "enabled"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k6-load-generator
  namespace: performance-testing
spec:
  replicas: 5
  selector:
    matchLabels:
      app: k6-load-generator
  template:
    metadata:
      labels:
        app: k6-load-generator
    spec:
      containers:
      - name: k6
        image: grafana/k6:latest
        env:
        - name: K6_PROMETHEUS_RW_SERVER_URL
          value: "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
        - name: K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM
          value: "true"
        command:
        - /bin/sh
        - -c
        - |
          cat > /tmp/load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend, Counter } from 'k6/metrics';
          
          // Custom metrics
          const errorRate = new Rate('error_rate');
          const responseTime = new Trend('response_time');
          const requestCount = new Counter('request_count');
          
          // Test configuration
          export const options = {
            stages: [
              { duration: '2m', target: 100 },   // Ramp up
              { duration: '5m', target: 100 },   // Stay at 100 users
              { duration: '2m', target: 200 },   // Ramp up to 200
              { duration: '5m', target: 200 },   // Stay at 200 users
              { duration: '2m', target: 500 },   // Ramp up to 500
              { duration: '10m', target: 500 },  // Stay at 500 users
              { duration: '5m', target: 1000 },  // Spike to 1000
              { duration: '5m', target: 1000 },  // Stay at 1000
              { duration: '5m', target: 0 },     // Ramp down
            ],
            thresholds: {
              http_req_duration: ['p(95)<500', 'p(99)<1000'],
              http_req_failed: ['rate<0.01'],
              error_rate: ['rate<0.01'],
            },
          };
          
          // Test scenarios
          export default function() {
            const scenarios = [
              { name: 'homepage', url: '/api/v1/health', weight: 30 },
              { name: 'user_login', url: '/api/v1/auth/login', weight: 20 },
              { name: 'product_search', url: '/api/v1/products/search', weight: 25 },
              { name: 'checkout', url: '/api/v1/checkout', weight: 15 },
              { name: 'user_profile', url: '/api/v1/users/profile', weight: 10 },
            ];
            
            // Select scenario based on weight
            const random = Math.random() * 100;
            let cumulative = 0;
            let selectedScenario;
            
            for (const scenario of scenarios) {
              cumulative += scenario.weight;
              if (random <= cumulative) {
                selectedScenario = scenario;
                break;
              }
            }
            
            // Execute request
            const response = http.get(`${__ENV.TARGET_URL}${selectedScenario.url}`, {
              headers: {
                'User-Agent': 'K6-LoadTest/1.0',
                'Accept': 'application/json',
              },
              tags: {
                scenario: selectedScenario.name,
              },
            });
            
            // Record metrics
            requestCount.add(1);
            responseTime.add(response.timings.duration);
            
            // Validate response
            const success = check(response, {
              'status is 200': (r) => r.status === 200,
              'response time < 500ms': (r) => r.timings.duration < 500,
              'response has body': (r) => r.body.length > 0,
            });
            
            errorRate.add(!success);
            
            // Realistic user behavior
            sleep(Math.random() * 3 + 1); // 1-4 seconds think time
          }
          
          // Setup function
          export function setup() {
            console.log('Starting performance test...');
            console.log(`Target URL: ${__ENV.TARGET_URL}`);
            console.log(`Test duration: ${options.stages.reduce((sum, stage) => sum + parseInt(stage.duration), 0)} minutes`);
          }
          
          // Teardown function
          export function teardown(data) {
            console.log('Performance test completed');
          }
          EOF
          
          # Run load test
          k6 run --out prometheus-rw /tmp/load-test.js
        env:
        - name: TARGET_URL
          value: "https://api.enterprise.com"
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
---
# JMeter Distributed Testing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jmeter-master
  namespace: performance-testing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jmeter-master
  template:
    metadata:
      labels:
        app: jmeter-master
    spec:
      containers:
      - name: jmeter
        image: justb4/jmeter:latest
        command:
        - /bin/bash
        - -c
        - |
          # Create JMeter test plan
          cat > /tmp/enterprise-load-test.jmx << 'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <jmeterTestPlan version="1.2">
            <hashTree>
              <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="Enterprise Load Test">
                <elementProp name="TestPlan.arguments" elementType="Arguments" guiclass="ArgumentsPanel">
                  <collectionProp name="Arguments.arguments"/>
                </elementProp>
                <stringProp name="TestPlan.user_define_classpath"></stringProp>
                <boolProp name="TestPlan.serialize_threadgroups">false</boolProp>
                <boolProp name="TestPlan.functional_mode">false</boolProp>
              </TestPlan>
              <hashTree>
                <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="Load Test Users">
                  <stringProp name="ThreadGroup.on_sample_error">continue</stringProp>
                  <elementProp name="ThreadGroup.main_controller" elementType="LoopController">
                    <boolProp name="LoopController.continue_forever">false</boolProp>
                    <intProp name="LoopController.loops">10</intProp>
                  </elementProp>
                  <stringProp name="ThreadGroup.num_threads">100</stringProp>
                  <stringProp name="ThreadGroup.ramp_time">300</stringProp>
                  <longProp name="ThreadGroup.start_time">1</longProp>
                  <longProp name="ThreadGroup.end_time">1</longProp>
                  <boolProp name="ThreadGroup.scheduler">false</boolProp>
                  <stringProp name="ThreadGroup.duration"></stringProp>
                  <stringProp name="ThreadGroup.delay"></stringProp>
                </ThreadGroup>
                <hashTree>
                  <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="API Health Check">
                    <elementProp name="HTTPsampler.Arguments" elementType="Arguments">
                      <collectionProp name="Arguments.arguments"/>
                    </elementProp>
                    <stringProp name="HTTPSampler.domain">api.enterprise.com</stringProp>
                    <stringProp name="HTTPSampler.port">443</stringProp>
                    <stringProp name="HTTPSampler.protocol">https</stringProp>
                    <stringProp name="HTTPSampler.path">/api/v1/health</stringProp>
                    <stringProp name="HTTPSampler.method">GET</stringProp>
                  </HTTPSamplerProxy>
                  <hashTree>
                    <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Response Code Assertion">
                      <collectionProp name="Asserion.test_strings">
                        <stringProp name="49586">200</stringProp>
                      </collectionProp>
                      <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
                      <boolProp name="Assertion.assume_success">false</boolProp>
                      <intProp name="Assertion.test_type">1</intProp>
                    </ResponseAssertion>
                    <hashTree/>
                  </hashTree>
                </hashTree>
              </hashTree>
            </hashTree>
          </jmeterTestPlan>
          EOF
          
          # Run JMeter test
          jmeter -n -t /tmp/enterprise-load-test.jmx -l /tmp/results.jtl -e -o /tmp/report
          
          # Keep container running
          tail -f /dev/null
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 4000m
            memory: 8Gi
```

### Performance Monitoring Stack
```yaml
# performance-monitoring.yaml - Comprehensive performance monitoring
apiVersion: apps/v1
kind: Deployment
metadata:
  name: performance-analyzer
  namespace: performance-testing
spec:
  replicas: 2
  selector:
    matchLabels:
      app: performance-analyzer
  template:
    metadata:
      labels:
        app: performance-analyzer
    spec:
      containers:
      - name: analyzer
        image: enterprise/performance-analyzer:v1.8.0
        env:
        - name: PROMETHEUS_URL
          value: "http://prometheus.monitoring.svc.cluster.local:9090"
        - name: GRAFANA_URL
          value: "http://grafana.monitoring.svc.cluster.local:3000"
        - name: ANALYSIS_INTERVAL
          value: "60s"
        command:
        - python3
        - -c
        - |
          import requests
          import json
          import time
          import numpy as np
          from datetime import datetime, timedelta
          import pandas as pd
          
          print("‚ö° Starting Performance Analyzer...")
          
          class PerformanceAnalyzer:
              def __init__(self):
                  self.prometheus_url = os.environ['PROMETHEUS_URL']
                  self.grafana_url = os.environ['GRAFANA_URL']
                  self.thresholds = {
                      'response_time_p95': 500,  # ms
                      'response_time_p99': 1000, # ms
                      'error_rate': 0.01,        # 1%
                      'cpu_utilization': 80,     # %
                      'memory_utilization': 85,  # %
                      'throughput_min': 1000     # req/s
                  }
              
              def query_prometheus(self, query, time_range='5m'):
                  """Query Prometheus for metrics"""
                  try:
                      response = requests.get(
                          f"{self.prometheus_url}/api/v1/query",
                          params={'query': query}
                      )
                      data = response.json()
                      if data['status'] == 'success' and data['data']['result']:
                          return float(data['data']['result'][0]['value'][1])
                      return None
                  except Exception as e:
                      print(f"Error querying Prometheus: {e}")
                      return None
              
              def analyze_response_times(self):
                  """Analyze response time metrics"""
                  metrics = {}
                  
                  # P95 response time
                  p95_query = 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))'
                  metrics['p95'] = self.query_prometheus(p95_query)
                  
                  # P99 response time
                  p99_query = 'histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))'
                  metrics['p99'] = self.query_prometheus(p99_query)
                  
                  # Average response time
                  avg_query = 'rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])'
                  metrics['avg'] = self.query_prometheus(avg_query)
                  
                  return metrics
              
              def analyze_throughput(self):
                  """Analyze throughput metrics"""
                  throughput_query = 'rate(http_requests_total[5m])'
                  return self.query_prometheus(throughput_query)
              
              def analyze_error_rates(self):
                  """Analyze error rate metrics"""
                  error_query = 'rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])'
                  return self.query_prometheus(error_query)
              
              def analyze_resource_utilization(self):
                  """Analyze CPU and memory utilization"""
                  metrics = {}
                  
                  # CPU utilization
                  cpu_query = '100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)'
                  metrics['cpu'] = self.query_prometheus(cpu_query)
                  
                  # Memory utilization
                  memory_query = '(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100'
                  metrics['memory'] = self.query_prometheus(memory_query)
                  
                  return metrics
              
              def identify_bottlenecks(self, metrics):
                  """Identify performance bottlenecks"""
                  bottlenecks = []
                  
                  # Response time bottlenecks
                  if metrics.get('response_times', {}).get('p95', 0) > self.thresholds['response_time_p95']:
                      bottlenecks.append({
                          'type': 'response_time',
                          'severity': 'high',
                          'metric': 'p95_response_time',
                          'current': metrics['response_times']['p95'],
                          'threshold': self.thresholds['response_time_p95'],
                          'recommendation': 'Scale up application instances or optimize database queries'
                      })
                  
                  # Error rate bottlenecks
                  if metrics.get('error_rate', 0) > self.thresholds['error_rate']:
                      bottlenecks.append({
                          'type': 'error_rate',
                          'severity': 'critical',
                          'metric': 'error_rate',
                          'current': metrics['error_rate'],
                          'threshold': self.thresholds['error_rate'],
                          'recommendation': 'Investigate application errors and fix bugs'
                      })
                  
                  # Resource utilization bottlenecks
                  if metrics.get('resources', {}).get('cpu', 0) > self.thresholds['cpu_utilization']:
                      bottlenecks.append({
                          'type': 'resource',
                          'severity': 'medium',
                          'metric': 'cpu_utilization',
                          'current': metrics['resources']['cpu'],
                          'threshold': self.thresholds['cpu_utilization'],
                          'recommendation': 'Scale up CPU resources or optimize CPU-intensive operations'
                      })
                  
                  if metrics.get('resources', {}).get('memory', 0) > self.thresholds['memory_utilization']:
                      bottlenecks.append({
                          'type': 'resource',
                          'severity': 'medium',
                          'metric': 'memory_utilization',
                          'current': metrics['resources']['memory'],
                          'threshold': self.thresholds['memory_utilization'],
                          'recommendation': 'Scale up memory or investigate memory leaks'
                      })
                  
                  # Throughput bottlenecks
                  if metrics.get('throughput', 0) < self.thresholds['throughput_min']:
                      bottlenecks.append({
                          'type': 'throughput',
                          'severity': 'medium',
                          'metric': 'throughput',
                          'current': metrics['throughput'],
                          'threshold': self.thresholds['throughput_min'],
                          'recommendation': 'Scale out application instances or optimize request handling'
                      })
                  
                  return bottlenecks
              
              def generate_performance_report(self, metrics, bottlenecks):
                  """Generate comprehensive performance report"""
                  report = {
                      'timestamp': datetime.utcnow().isoformat(),
                      'summary': {
                          'overall_health': 'healthy' if len(bottlenecks) == 0 else 'degraded',
                          'bottlenecks_count': len(bottlenecks),
                          'critical_issues': len([b for b in bottlenecks if b['severity'] == 'critical'])
                      },
                      'metrics': metrics,
                      'bottlenecks': bottlenecks,
                      'recommendations': []
                  }
                  
                  # Generate recommendations
                  if len(bottlenecks) == 0:
                      report['recommendations'].append("System performance is optimal")
                  else:
                      for bottleneck in bottlenecks:
                          report['recommendations'].append(bottleneck['recommendation'])
                  
                  return report
              
              def send_performance_alert(self, report):
                  """Send performance alert if issues detected"""
                  if report['summary']['critical_issues'] > 0:
                      alert_data = {
                          'title': 'üö® Critical Performance Issues Detected',
                          'severity': 'critical',
                          'issues': report['summary']['critical_issues'],
                          'bottlenecks': [b for b in report['bottlenecks'] if b['severity'] == 'critical']
                      }
                      
                      # Send to alerting system
                      print(f"üö® CRITICAL ALERT: {alert_data['issues']} performance issues detected")
                      
                  elif len(report['bottlenecks']) > 0:
                      print(f"‚ö†Ô∏è WARNING: {len(report['bottlenecks'])} performance bottlenecks detected")
              
              def run_analysis(self):
                  """Run complete performance analysis"""
                  print("üìä Running performance analysis...")
                  
                  # Collect metrics
                  metrics = {
                      'response_times': self.analyze_response_times(),
                      'throughput': self.analyze_throughput(),
                      'error_rate': self.analyze_error_rates(),
                      'resources': self.analyze_resource_utilization()
                  }
                  
                  # Identify bottlenecks
                  bottlenecks = self.identify_bottlenecks(metrics)
                  
                  # Generate report
                  report = self.generate_performance_report(metrics, bottlenecks)
                  
                  # Send alerts if needed
                  self.send_performance_alert(report)
                  
                  # Display summary
                  print(f"üìà Performance Summary:")
                  print(f"  Response Time P95: {metrics['response_times'].get('p95', 'N/A')}ms")
                  print(f"  Throughput: {metrics.get('throughput', 'N/A')} req/s")
                  print(f"  Error Rate: {metrics.get('error_rate', 'N/A')*100:.2f}%")
                  print(f"  CPU Utilization: {metrics['resources'].get('cpu', 'N/A')}%")
                  print(f"  Memory Utilization: {metrics['resources'].get('memory', 'N/A')}%")
                  print(f"  Bottlenecks: {len(bottlenecks)}")
                  
                  return report
          
          # Main execution
          analyzer = PerformanceAnalyzer()
          
          while True:
              try:
                  report = analyzer.run_analysis()
                  print("‚úÖ Performance analysis completed")
                  
              except Exception as e:
                  print(f"‚ùå Error in performance analysis: {e}")
              
              time.sleep(int(os.environ['ANALYSIS_INTERVAL'].rstrip('s')))
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 2Gi
```

## üîß PowerShell Performance Engineering Scripts

### Complete Performance Testing Suite
```powershell
# performance-engineering.ps1 - Complete performance testing automation
param(
    [Parameter(Mandatory=$true)]
    [ValidateSet("load-test", "stress-test", "capacity-plan", "optimize")]
    [string]$Action,
    
    [string]$TargetUrl = "https://api.enterprise.com",
    [int]$MaxUsers = 1000,
    [int]$TestDuration = 300,  # seconds
    [decimal]$ResponseTimeThreshold = 500,  # ms
    [decimal]$ErrorRateThreshold = 0.01     # 1%
)

Write-Host "‚ö° Performance Engineering Automation - $Action" -ForegroundColor Green

# Function to run K6 load test
function Invoke-K6LoadTest {
    param(
        [string]$Url,
        [int]$Users,
        [int]$Duration
    )
    
    Write-Host "üöÄ Running K6 load test..." -ForegroundColor Yellow
    Write-Host "  Target: $Url" -ForegroundColor Gray
    Write-Host "  Users: $Users" -ForegroundColor Gray
    Write-Host "  Duration: $Duration seconds" -ForegroundColor Gray
    
    # Create K6 test script
    $k6Script = @"
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

const errorRate = new Rate('error_rate');
const responseTime = new Trend('response_time');

export const options = {
  stages: [
    { duration: '30s', target: $($Users / 4) },
    { duration: '1m', target: $($Users / 2) },
    { duration: '2m', target: $Users },
    { duration: '$($Duration - 210)s', target: $Users },
    { duration: '30s', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<$ResponseTimeThreshold'],
    http_req_failed: ['rate<$ErrorRateThreshold'],
  },
};

export default function() {
  const endpoints = [
    { url: '/api/v1/health', weight: 30 },
    { url: '/api/v1/users', weight: 25 },
    { url: '/api/v1/products', weight: 20 },
    { url: '/api/v1/orders', weight: 15 },
    { url: '/api/v1/analytics', weight: 10 },
  ];
  
  const endpoint = endpoints[Math.floor(Math.random() * endpoints.length)];
  
  const response = http.get('$Url' + endpoint.url, {
    headers: {
      'User-Agent': 'K6-Performance-Test/1.0',
    },
  });
  
  responseTime.add(response.timings.duration);
  
  const success = check(response, {
    'status is 200': (r) => r.status === 200,
    'response time OK': (r) => r.timings.duration < $ResponseTimeThreshold,
  });
  
  errorRate.add(!success);
  
  sleep(Math.random() * 2 + 1);
}
"@
    
    # Save script to file
    $scriptPath = "k6-load-test.js"
    $k6Script | Out-File -FilePath $scriptPath -Encoding UTF8
    
    try {
        # Run K6 test
        $result = k6 run --out json=results.json $scriptPath
        
        if (Test-Path "results.json") {
            $results = Get-Content "results.json" | ConvertFrom-Json
            
            # Parse results
            $metrics = @{
                TotalRequests = ($results | Where-Object { $_.type -eq "Point" -and $_.metric -eq "http_reqs" } | Measure-Object -Property value -Sum).Sum
                AvgResponseTime = ($results | Where-Object { $_.type -eq "Point" -and $_.metric -eq "http_req_duration" } | Measure-Object -Property value -Average).Average
                ErrorRate = ($results | Where-Object { $_.type -eq "Point" -and $_.metric -eq "http_req_failed" } | Measure-Object -Property value -Average).Average
                P95ResponseTime = 0  # Simplified for demo
                Throughput = 0       # Simplified for demo
            }
            
            Write-Host "üìä Load Test Results:" -ForegroundColor Green
            Write-Host "  Total Requests: $($metrics.TotalRequests)" -ForegroundColor White
            Write-Host "  Avg Response Time: $($metrics.AvgResponseTime.ToString('F2'))ms" -ForegroundColor White
            Write-Host "  Error Rate: $($metrics.ErrorRate.ToString('P2'))" -ForegroundColor White
            
            return $metrics
        }
        
    } catch {
        Write-Host "‚ùå K6 load test failed: $($_.Exception.Message)" -ForegroundColor Red
        return $null
    } finally {
        # Cleanup
        if (Test-Path $scriptPath) { Remove-Item $scriptPath }
        if (Test-Path "results.json") { Remove-Item "results.json" }
    }
}

# Function to run stress test
function Invoke-StressTest {
    param(
        [string]$Url,
        [int]$MaxUsers
    )
    
    Write-Host "üí• Running stress test to find breaking point..." -ForegroundColor Yellow
    
    $breakingPoint = 0
    $currentUsers = 50
    $increment = 50
    
    while ($currentUsers -le $MaxUsers) {
        Write-Host "  Testing with $currentUsers users..." -ForegroundColor Gray
        
        $results = Invoke-K6LoadTest -Url $Url -Users $currentUsers -Duration 120
        
        if ($results) {
            if ($results.ErrorRate -gt $ErrorRateThreshold -or $results.AvgResponseTime -gt $ResponseTimeThreshold) {
                $breakingPoint = $currentUsers
                Write-Host "üí• Breaking point found at $breakingPoint users" -ForegroundColor Red
                break
            }
        }
        
        $currentUsers += $increment
        Start-Sleep -Seconds 30  # Cool down between tests
    }
    
    if ($breakingPoint -eq 0) {
        Write-Host "‚úÖ System handled maximum test load ($MaxUsers users)" -ForegroundColor Green
        $breakingPoint = $MaxUsers
    }
    
    return @{
        BreakingPoint = $breakingPoint
        MaxSafeLoad = [Math]::Floor($breakingPoint * 0.8)  # 80% of breaking point
        RecommendedCapacity = [Math]::Floor($breakingPoint * 0.6)  # 60% for normal operations
    }
}

# Function to perform capacity planning
function Invoke-CapacityPlanning {
    param(
        [string]$Url,
        [int]$CurrentUsers,
        [decimal]$GrowthRate = 0.2  # 20% growth
    )
    
    Write-Host "üìà Performing capacity planning analysis..." -ForegroundColor Yellow
    
    # Current performance baseline
    Write-Host "  Establishing baseline performance..." -ForegroundColor Gray
    $baseline = Invoke-K6LoadTest -Url $Url -Users $CurrentUsers -Duration 180
    
    if (-not $baseline) {
        Write-Host "‚ùå Failed to establish baseline" -ForegroundColor Red
        return
    }
    
    # Project future load
    $projections = @()
    for ($months = 3; $months -le 12; $months += 3) {
        $projectedUsers = [Math]::Ceiling($CurrentUsers * [Math]::Pow(1 + $GrowthRate, $months / 12))
        $projections += @{
            Months = $months
            ProjectedUsers = $projectedUsers
            EstimatedResponseTime = $baseline.AvgResponseTime * ($projectedUsers / $CurrentUsers)
            RecommendedInstances = [Math]::Ceiling($projectedUsers / ($CurrentUsers / 3))  # Assuming 3 current instances
        }
    }
    
    Write-Host "üìä Capacity Planning Results:" -ForegroundColor Green
    Write-Host "  Current Baseline:" -ForegroundColor White
    Write-Host "    Users: $CurrentUsers" -ForegroundColor Gray
    Write-Host "    Avg Response Time: $($baseline.AvgResponseTime.ToString('F2'))ms" -ForegroundColor Gray
    Write-Host "    Error Rate: $($baseline.ErrorRate.ToString('P2'))" -ForegroundColor Gray
    
    Write-Host "  Future Projections:" -ForegroundColor White
    foreach ($projection in $projections) {
        Write-Host "    $($projection.Months) months: $($projection.ProjectedUsers) users, $($projection.EstimatedResponseTime.ToString('F2'))ms response time, $($projection.RecommendedInstances) instances" -ForegroundColor Gray
    }
    
    return @{
        Baseline = $baseline
        Projections = $projections
        Recommendations = @(
            "Monitor response times closely as load increases",
            "Plan infrastructure scaling 2-3 months in advance",
            "Consider implementing auto-scaling policies",
            "Regular performance testing recommended"
        )
    }
}

# Function to optimize performance
function Invoke-PerformanceOptimization {
    param([string]$Url)
    
    Write-Host "üéØ Running performance optimization analysis..." -ForegroundColor Yellow
    
    $optimizations = @()
    
    # Test different scenarios
    $scenarios = @(
        @{ Name = "Current"; Users = 100; Duration = 120 },
        @{ Name = "With Caching"; Users = 100; Duration = 120 },  # Simulated
        @{ Name = "Optimized DB"; Users = 100; Duration = 120 }   # Simulated
    )
    
    foreach ($scenario in $scenarios) {
        Write-Host "  Testing scenario: $($scenario.Name)..." -ForegroundColor Gray
        
        $results = Invoke-K6LoadTest -Url $Url -Users $scenario.Users -Duration $scenario.Duration
        
        if ($results) {
            $optimizations += @{
                Scenario = $scenario.Name
                ResponseTime = $results.AvgResponseTime
                ErrorRate = $results.ErrorRate
                Throughput = $results.TotalRequests / ($scenario.Duration / 60)  # req/min
            }
        }
    }
    
    # Generate optimization recommendations
    $recommendations = @(
        "üöÄ Enable response caching for static content",
        "üóÑÔ∏è Optimize database queries and add indexes",
        "‚ö° Implement connection pooling",
        "üì¶ Use CDN for static assets",
        "üîÑ Enable gzip compression",
        "üéØ Implement lazy loading for heavy operations",
        "üìä Add database read replicas",
        "üèóÔ∏è Consider microservices architecture for bottlenecks"
    )
    
    Write-Host "üéØ Performance Optimization Results:" -ForegroundColor Green
    foreach ($opt in $optimizations) {
        Write-Host "  $($opt.Scenario): $($opt.ResponseTime.ToString('F2'))ms avg, $($opt.ErrorRate.ToString('P2')) errors, $($opt.Throughput.ToString('F0')) req/min" -ForegroundColor White
    }
    
    Write-Host "üí° Optimization Recommendations:" -ForegroundColor Green
    foreach ($rec in $recommendations) {
        Write-Host "  $rec" -ForegroundColor White
    }
    
    return @{
        Results = $optimizations
        Recommendations = $recommendations
    }
}

# Main execution
try {
    switch ($Action) {
        "load-test" {
            $results = Invoke-K6LoadTest -Url $TargetUrl -Users $MaxUsers -Duration $TestDuration
            if ($results) {
                Write-Host "üéâ Load test completed successfully" -ForegroundColor Green
                
                # Check if performance meets thresholds
                if ($results.AvgResponseTime -gt $ResponseTimeThreshold) {
                    Write-Host "‚ö†Ô∏è Response time exceeds threshold ($ResponseTimeThreshold ms)" -ForegroundColor Yellow
                }
                if ($results.ErrorRate -gt $ErrorRateThreshold) {
                    Write-Host "‚ö†Ô∏è Error rate exceeds threshold ($($ErrorRateThreshold.ToString('P2')))" -ForegroundColor Yellow
                }
            }
        }
        
        "stress-test" {
            $stressResults = Invoke-StressTest -Url $TargetUrl -MaxUsers $MaxUsers
            Write-Host "üí• Stress Test Results:" -ForegroundColor Green
            Write-Host "  Breaking Point: $($stressResults.BreakingPoint) users" -ForegroundColor White
            Write-Host "  Max Safe Load: $($stressResults.MaxSafeLoad) users" -ForegroundColor White
            Write-Host "  Recommended Capacity: $($stressResults.RecommendedCapacity) users" -ForegroundColor White
        }
        
        "capacity-plan" {
            $capacityResults = Invoke-CapacityPlanning -Url $TargetUrl -CurrentUsers ($MaxUsers / 2)
            Write-Host "üìà Capacity planning analysis completed" -ForegroundColor Green
        }
        
        "optimize" {
            $optimizationResults = Invoke-PerformanceOptimization -Url $TargetUrl
            Write-Host "üéØ Performance optimization analysis completed" -ForegroundColor Green
        }
    }
    
} catch {
    Write-Host "‚ùå Performance engineering failed: $($_.Exception.Message)" -ForegroundColor Red
    exit 1
}
```

## üèÜ Enterprise Performance Success Stories

### Amazon - 100ms = $1.6B Impact
**Challenge:** Optimize performance for global e-commerce platform
**Strategy:**
- Comprehensive load testing for all services
- Real-time performance monitoring
- Predictive capacity planning
- Automated performance optimization

**Results:**
- 100ms latency improvement = $1.6B revenue increase
- 99.99% uptime during peak events
- 50% reduction in infrastructure costs
- Proactive scaling prevents outages

### Netflix - $1B Infrastructure Savings
**Challenge:** Optimize streaming performance for 200M+ users
**Strategy:**
- Chaos engineering for resilience testing
- ML-powered capacity planning
- Edge caching optimization
- Real-time performance analytics

**Results:**
- $1B+ savings through performance optimization
- 99.9% streaming quality maintained
- 40% reduction in CDN costs
- Seamless 4K streaming globally

---

**Master Performance Engineering and deliver lightning-fast experiences at enterprise scale!** ‚ö°